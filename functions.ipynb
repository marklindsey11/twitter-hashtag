{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from deep_translator import GoogleTranslator\n",
    "import my_tokens\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "import regex as re\n",
    "import tweepy\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import boto3\n",
    "import tensorflow_hub as hub\n",
    "from scipy.special import softmax\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2 \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(my_tokens.API_KEY, my_tokens.API_SECRET)\n",
    "auth.set_access_token(my_tokens.ACCESS_TOKEN, my_tokens.ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "client = tweepy.Client(bearer_token=my_tokens.BEARER_TOKEN, consumer_key=my_tokens.API_KEY, consumer_secret=my_tokens.API_SECRET, access_token=my_tokens.ACCESS_TOKEN, access_token_secret=my_tokens.ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tt = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module universal-sentence-encoder_4 loaded\n"
     ]
    }
   ],
   "source": [
    "model_path = \"universal-sentence-encoder_4\"\n",
    "model = hub.load(model_path)\n",
    "print (\"module %s loaded\" % model_path)\n",
    "def get_embeding(input):\n",
    "  return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Tweets(filename, query, tweet_field=None, user_field = None, start_date=None, end_date=None):\n",
    "    \n",
    "    print(start_date, end_date)\n",
    "    tweets = tweepy.Paginator(\n",
    "        client.search_all_tweets, \n",
    "        query=query, \n",
    "        max_results=500, \n",
    "        start_time=start_date, \n",
    "        user_fields=user_field, \n",
    "        expansions=['author_id','entities.mentions.username'],\n",
    "        tweet_fields=tweet_field).flatten()\n",
    "\n",
    "    tweets_for_csv = []\n",
    "    for tweet in tweets:\n",
    "        all_mentions = []\n",
    "        ex_links = []\n",
    "        all_hashtags = []\n",
    "        if tweet.entities:\n",
    "            if 'urls' in tweet.entities:\n",
    "                for link in tweet.entities['urls']:\n",
    "                    ex_links.append(link['expanded_url'])\n",
    "            if 'mentions' in tweet.entities:\n",
    "                for mention in tweet.entities['mentions']:\n",
    "                    all_mentions.append(mention['username'])\n",
    "            if 'hashtags' in tweet.entities:\n",
    "                for mention in tweet.entities['hashtags']:\n",
    "                    all_hashtags.append(mention['tag'])\n",
    "        tweets_for_csv.append([tweet.id, tweet.author_id, tweet.source,tweet.created_at, tweet.text, tweet.lang, ex_links, all_mentions, all_hashtags])\n",
    "    outfile = filename + \".csv\"\n",
    "    print(\"writing to \" + outfile)\n",
    "    with open(outfile, 'w+') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerows(tweets_for_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_info(filename, user_ids, user_field=None):\n",
    "\n",
    "    index = 0\n",
    "    users_for_csv = []\n",
    "    while index < len(user_ids):\n",
    "        uids = user_ids[index : min(index+100, len(user_ids))]\n",
    "        users = client.get_users(ids=uids, user_fields=user_field)\n",
    "        for user in users.data:\n",
    "            users_for_csv.append([user.id, user.created_at ,user.name, user.username, user.verified, user.protected, user.public_metrics['followers_count'], user.public_metrics['following_count'], user.public_metrics['tweet_count']])\n",
    "        index += 100\n",
    "\n",
    "    outfile = filename + \".csv\"\n",
    "    print(\"writing to \" + outfile)\n",
    "    with open(outfile, 'w+') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerows(users_for_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeline(filename, ids, end_date=None, tweet_field=None, lim=200):\n",
    "\n",
    "    user_timeline = []\n",
    "    for idx in ids:\n",
    "        tweets = tweepy.Paginator(\n",
    "            client.get_users_tweets,\n",
    "            id=idx, \n",
    "            max_results=100, \n",
    "            tweet_fields=tweet_field,\n",
    "            expansions=['referenced_tweets.id'],\n",
    "            exclude=['retweets'],\n",
    "            end_time = end_date).flatten(limit=lim)\n",
    "        # for t in tweets:\n",
    "        for tweet in tweets:\n",
    "            user_timeline.append([idx, tweet.id, tweet.source, tweet.created_at, tweet.text, tweet.lang,tweet.public_metrics['like_count'], tweet.public_metrics['retweet_count']])\n",
    "            \n",
    "    outfile = filename + \".csv\"\n",
    "    print(\"writing to \" + outfile)\n",
    "    with open(outfile, 'w+') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerows(user_timeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_lookup(df, ids,media_fields=None, tweet_fields=None, user_fields=None, expansions=None):\n",
    "    column_links = []\n",
    "    column_type = []\n",
    "    index = 0\n",
    "    while index < len(ids):\n",
    "        idx = ids[index : min(index+100, len(ids))]\n",
    "        tweets = client.get_tweets(ids=idx,media_fields=media_fields, user_fields=user_fields, tweet_fields=tweet_fields, expansions=expansions)\n",
    "        ridx = []\n",
    "        # print(len(idx), len(tweets.data))\n",
    "        for tweet in tweets.data:\n",
    "            ridx.append(tweet.id)\n",
    "            row_url = []\n",
    "            row_type = []\n",
    "            if tweets.includes and tweet.attachments:\n",
    "                if 'media' in tweets.includes and 'media_keys' in tweet.attachments:\n",
    "                    for media in tweets.includes['media']:\n",
    "                        if media.media_key in tweet.attachments['media_keys']:\n",
    "                            row_url.append(media.url)\n",
    "                            row_type.append(media.type)\n",
    "            column_links.append(row_url)\n",
    "            column_type.append(row_type)\n",
    "        \n",
    "        # print(idx)\n",
    "        # print(ridx)\n",
    "        # print()\n",
    "        for i in range(len(idx)):\n",
    "            if ridx[i] != idx[i]:\n",
    "                column_links.insert(-1,i)\n",
    "                column_type.insert(-1,i)\n",
    "                ridx.insert(i,idx[i])\n",
    "        index += 100\n",
    "            \n",
    "    df['media_link'] = column_links\n",
    "    df['media_type'] = column_type\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(lst, word):\n",
    "    cnt = 0\n",
    "    for x in [s.lower() for s in lst]:\n",
    "        if x == word.lower():\n",
    "            cnt+=1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_ratio(data, followers, following):\n",
    "    data['f_ratio'] = data[followers]/ np.clip(data[following], 1e-7, 1e10)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "    text = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stopWords(text):\n",
    "    return \" \".join([w.lower() for w in text.split() if w.lower() not in stop_words and len(w) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tt.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    text = [lemmatizer.lemmatize(token) for token in text]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translate(text):\n",
    "    translation = GoogleTranslator(source='auto', target='en').translate(text=text)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppm_count(lst):\n",
    "    cnt = 0\n",
    "    l = [s.lower() for s in lst]\n",
    "    for x in range(len(l)-1):\n",
    "        if l[x] == 'ppm':\n",
    "            cnt+=1\n",
    "\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    return sentiment_task(text)[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageResizeTrain(image):\n",
    "    maxD = 1024\n",
    "    height,width = image.shape\n",
    "    aspectRatio = width/height\n",
    "    if aspectRatio < 1:\n",
    "        newSize = (int(maxD*aspectRatio),maxD)\n",
    "    else:\n",
    "        newSize = (maxD,int(maxD/aspectRatio))\n",
    "    image = cv2.resize(image,newSize)\n",
    "    return image\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "def computeSIFT(image):\n",
    "    return sift.detectAndCompute(image, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher()\n",
    "def calculateMatches(des1,des2):\n",
    "    matches = bf.knnMatch(des1,des2,k=2)\n",
    "    topResults1 = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            topResults1.append([m])\n",
    "            \n",
    "    matches = bf.knnMatch(des2,des1,k=2)\n",
    "    topResults2 = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            topResults2.append([m])\n",
    "    \n",
    "    topResults = []\n",
    "    for match1 in topResults1:\n",
    "        match1QueryIndex = match1[0].queryIdx\n",
    "        match1TrainIndex = match1[0].trainIdx\n",
    "\n",
    "        for match2 in topResults2:\n",
    "            match2QueryIndex = match2[0].queryIdx\n",
    "            match2TrainIndex = match2[0].trainIdx\n",
    "\n",
    "            if (match1QueryIndex == match2TrainIndex) and (match1TrainIndex == match2QueryIndex):\n",
    "                topResults.append(match1)\n",
    "    return topResults\n",
    "\n",
    "def calculateScore(matches,keypoint1,keypoint2):\n",
    "    return 100 * (matches/min(keypoint1,keypoint2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPlot(image1,image2,keypoint1,keypoint2,matches):\n",
    "    image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    matchPlot = cv2.drawMatchesKnn(image1,keypoint1,image2,keypoint2,matches,None,[255,255,255],flags=2)\n",
    "    return matchPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateResultsFor(imageA,imageB):\n",
    "    img1 = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    "    img1 = imageResizeTrain(img1)\n",
    "    img2 = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "    img2 = imageResizeTrain(img2)\n",
    "    keypoint1 , descriptor1 = computeSIFT(img1)\n",
    "    keypoint2, descriptor2 = computeSIFT(img2)\n",
    "    matches = calculateMatches(descriptor1, descriptor2)\n",
    "    score = calculateScore(len(matches),len(keypoint1),len(keypoint2))\n",
    "    if score < 10 or score == 100:\n",
    "        return \n",
    "    fx, ax = plt.subplots(1,2, figsize=(16,10))\n",
    "    # plt.figure(figsize=(16,10))\n",
    "    plt.title(str(score)+'%'+' Similar')\n",
    "    ax[0].imshow(cv2.cvtColor(imageA, cv2.COLOR_BGR2RGB))\n",
    "    ax[1].imshow(cv2.cvtColor(imageB, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_followers(id, filepath, maxres = 10000):\n",
    "    followers = tweepy.Paginator(client.get_users_followers,id, max_results = 1000).flatten(maxres)\n",
    "    outfile = filepath\n",
    "    rows = []\n",
    "    if followers:\n",
    "        for follower in followers:\n",
    "            rows.append([follower.id, follower.username, follower.name])\n",
    "    print(\"writing to \" + outfile)\n",
    "    with open(outfile, 'w+') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_following(id, filepath, maxres = 10000):\n",
    "    followings = tweepy.Paginator(client.get_users_following,id, max_results = 1000).flatten(maxres)\n",
    "    outfile = filepath\n",
    "    rows = []\n",
    "    if followings:\n",
    "        for following in followings:\n",
    "            rows.append([following.id, following.username, following.name])\n",
    "    print(\"writing to \" + outfile)\n",
    "    with open(outfile, 'w+') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f469e64d41316cfc7982ed75d75d7005fb2f2934718a7d5882ff9f107673014a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
